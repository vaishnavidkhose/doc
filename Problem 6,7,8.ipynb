{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec470bf",
   "metadata": {},
   "source": [
    "## Name: Vaibhav Bichave\n",
    "\n",
    "## Implement the Continuous Bag of Words (CBOW) Model for the given (textual document) using the below steps:\n",
    "    a. Data preparation\n",
    "    b. Generate training data\n",
    "    c. Train model\n",
    "    d. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9467d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =\"\"\"But I must explain to you how all this mistaken idea of denouncing pleasure and praising pain \n",
    "was born and I will give you a complete account of the system, and expound the actual teachings \n",
    "of the great explorer of the truth, the master-builder of human happiness. No one rejects, \n",
    "dislikes, or avoids pleasure itself, because it is pleasure, but because those who do not know \n",
    "how to pursue pleasure rationally encounter consequences that are extremely painful. Nor again \n",
    "is there anyone who loves or pursues or desires to obtain pain of itself, because it is pain, \n",
    "but because occasionally circumstances occur in which toil and pain can procure him some great \n",
    "pleasure. To take a trivial example, which of us ever undertakes laborious physical exercise, \n",
    "except to obtain some advantage from it? But who has any right to find fault with a man who \n",
    "chooses to enjoy a pleasure that has no annoying consequences, or one who avoids a pain that \n",
    "produces no resultant pleasure?\"\"\"\n",
    "\n",
    "data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9e67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "word2id['PAD'] = 0\n",
    "\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "wids = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "emb_size = 100\n",
    "window_size = 2\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63df0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca027e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_model(corpus,vocab_size, window_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sequences_size = len(words)\n",
    "        for index,word in enumerate(words):\n",
    "            context_word = []\n",
    "            label_word = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            context_word.append([words[i]\n",
    "                               for i in range(start,end)\n",
    "                               if 0<=i <sequences_size\n",
    "                               and i!=index])\n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = pad_sequences(context_word,context_length)\n",
    "            y = to_categorical(label_word,vocab_size)\n",
    "            yield(x,y)\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3947598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,Lambda\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce96afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 100)            10200     \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 102)               10302     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,502\n",
      "Trainable params: 20,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential([\n",
    "    Embedding(vocab_size,emb_size,input_length = window_size*2),\n",
    "    Lambda(lambda x:K.mean(x,axis=1)),\n",
    "    Dense(vocab_size,activation = 'softmax')\n",
    "])\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d5e66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0 - Loss -> 775.3729386329651\n",
      "Epochs 1 - Loss -> 762.764808177948\n",
      "Epochs 2 - Loss -> 752.3622903823853\n",
      "Epochs 3 - Loss -> 743.9978125095367\n",
      "Epochs 4 - Loss -> 739.9645841121674\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(5):\n",
    "    loss  = 0\n",
    "    for x,y in cbow_model(corpus=wids,vocab_size = vocab_size,window_size=window_size):\n",
    "        loss += cbow.train_on_batch(x,y)\n",
    "    print(\"Epochs {} - Loss -> {}\".format(epochs,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b2333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "weights = cbow.get_weights()[0][:]\n",
    "# pd.DataFrame(weights,index=word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa517f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>to</th>\n",
       "      <th>of</th>\n",
       "      <th>pleasure</th>\n",
       "      <th>pain</th>\n",
       "      <th>a</th>\n",
       "      <th>the</th>\n",
       "      <th>who</th>\n",
       "      <th>but</th>\n",
       "      <th>and</th>\n",
       "      <th>or</th>\n",
       "      <th>...</th>\n",
       "      <th>find</th>\n",
       "      <th>fault</th>\n",
       "      <th>with</th>\n",
       "      <th>man</th>\n",
       "      <th>chooses</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>annoying</th>\n",
       "      <th>produces</th>\n",
       "      <th>resultant</th>\n",
       "      <th>PAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.787391</td>\n",
       "      <td>0.810214</td>\n",
       "      <td>0.827404</td>\n",
       "      <td>0.797251</td>\n",
       "      <td>0.814888</td>\n",
       "      <td>0.786212</td>\n",
       "      <td>0.853085</td>\n",
       "      <td>0.804344</td>\n",
       "      <td>0.820375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778528</td>\n",
       "      <td>0.810712</td>\n",
       "      <td>0.812098</td>\n",
       "      <td>0.789316</td>\n",
       "      <td>0.770183</td>\n",
       "      <td>0.826989</td>\n",
       "      <td>0.780721</td>\n",
       "      <td>0.832445</td>\n",
       "      <td>0.785573</td>\n",
       "      <td>0.789727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.787391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384531</td>\n",
       "      <td>0.416828</td>\n",
       "      <td>0.325038</td>\n",
       "      <td>0.406923</td>\n",
       "      <td>0.388624</td>\n",
       "      <td>0.438588</td>\n",
       "      <td>0.389750</td>\n",
       "      <td>0.426883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425372</td>\n",
       "      <td>0.401584</td>\n",
       "      <td>0.444521</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>0.373554</td>\n",
       "      <td>0.398741</td>\n",
       "      <td>0.408482</td>\n",
       "      <td>0.411565</td>\n",
       "      <td>0.440255</td>\n",
       "      <td>0.405608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleasure</th>\n",
       "      <td>0.810214</td>\n",
       "      <td>0.384531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409522</td>\n",
       "      <td>0.375739</td>\n",
       "      <td>0.403045</td>\n",
       "      <td>0.407210</td>\n",
       "      <td>0.402263</td>\n",
       "      <td>0.393062</td>\n",
       "      <td>0.427232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429522</td>\n",
       "      <td>0.404031</td>\n",
       "      <td>0.399282</td>\n",
       "      <td>0.409312</td>\n",
       "      <td>0.405998</td>\n",
       "      <td>0.394687</td>\n",
       "      <td>0.416837</td>\n",
       "      <td>0.403753</td>\n",
       "      <td>0.441222</td>\n",
       "      <td>0.411379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pain</th>\n",
       "      <td>0.827404</td>\n",
       "      <td>0.416828</td>\n",
       "      <td>0.409522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391088</td>\n",
       "      <td>0.427218</td>\n",
       "      <td>0.427060</td>\n",
       "      <td>0.412869</td>\n",
       "      <td>0.367085</td>\n",
       "      <td>0.401413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407849</td>\n",
       "      <td>0.381329</td>\n",
       "      <td>0.419598</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.424888</td>\n",
       "      <td>0.378397</td>\n",
       "      <td>0.351677</td>\n",
       "      <td>0.332776</td>\n",
       "      <td>0.430496</td>\n",
       "      <td>0.420382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.797251</td>\n",
       "      <td>0.325038</td>\n",
       "      <td>0.375739</td>\n",
       "      <td>0.391088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377490</td>\n",
       "      <td>0.405434</td>\n",
       "      <td>0.418239</td>\n",
       "      <td>0.381655</td>\n",
       "      <td>0.385599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419062</td>\n",
       "      <td>0.340630</td>\n",
       "      <td>0.443746</td>\n",
       "      <td>0.377340</td>\n",
       "      <td>0.369278</td>\n",
       "      <td>0.441894</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.377152</td>\n",
       "      <td>0.433487</td>\n",
       "      <td>0.366327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enjoy</th>\n",
       "      <td>0.826989</td>\n",
       "      <td>0.398741</td>\n",
       "      <td>0.394687</td>\n",
       "      <td>0.378397</td>\n",
       "      <td>0.441894</td>\n",
       "      <td>0.410658</td>\n",
       "      <td>0.391083</td>\n",
       "      <td>0.406363</td>\n",
       "      <td>0.407552</td>\n",
       "      <td>0.428812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429945</td>\n",
       "      <td>0.439026</td>\n",
       "      <td>0.420391</td>\n",
       "      <td>0.400164</td>\n",
       "      <td>0.428240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424172</td>\n",
       "      <td>0.416142</td>\n",
       "      <td>0.399358</td>\n",
       "      <td>0.440132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoying</th>\n",
       "      <td>0.780721</td>\n",
       "      <td>0.408482</td>\n",
       "      <td>0.416837</td>\n",
       "      <td>0.351677</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.438933</td>\n",
       "      <td>0.426726</td>\n",
       "      <td>0.446179</td>\n",
       "      <td>0.385946</td>\n",
       "      <td>0.389236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415694</td>\n",
       "      <td>0.452983</td>\n",
       "      <td>0.443795</td>\n",
       "      <td>0.378387</td>\n",
       "      <td>0.435288</td>\n",
       "      <td>0.424172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379482</td>\n",
       "      <td>0.453137</td>\n",
       "      <td>0.388455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>produces</th>\n",
       "      <td>0.832445</td>\n",
       "      <td>0.411565</td>\n",
       "      <td>0.403753</td>\n",
       "      <td>0.332776</td>\n",
       "      <td>0.377152</td>\n",
       "      <td>0.422260</td>\n",
       "      <td>0.434561</td>\n",
       "      <td>0.405213</td>\n",
       "      <td>0.405003</td>\n",
       "      <td>0.406478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415976</td>\n",
       "      <td>0.410710</td>\n",
       "      <td>0.432731</td>\n",
       "      <td>0.432116</td>\n",
       "      <td>0.419045</td>\n",
       "      <td>0.416142</td>\n",
       "      <td>0.379482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425729</td>\n",
       "      <td>0.378775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resultant</th>\n",
       "      <td>0.785573</td>\n",
       "      <td>0.440255</td>\n",
       "      <td>0.441222</td>\n",
       "      <td>0.430496</td>\n",
       "      <td>0.433487</td>\n",
       "      <td>0.439111</td>\n",
       "      <td>0.429622</td>\n",
       "      <td>0.434378</td>\n",
       "      <td>0.363705</td>\n",
       "      <td>0.383129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381278</td>\n",
       "      <td>0.406512</td>\n",
       "      <td>0.433371</td>\n",
       "      <td>0.401329</td>\n",
       "      <td>0.417583</td>\n",
       "      <td>0.399358</td>\n",
       "      <td>0.453137</td>\n",
       "      <td>0.425729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAD</th>\n",
       "      <td>0.789727</td>\n",
       "      <td>0.405608</td>\n",
       "      <td>0.411379</td>\n",
       "      <td>0.420382</td>\n",
       "      <td>0.366327</td>\n",
       "      <td>0.409461</td>\n",
       "      <td>0.399728</td>\n",
       "      <td>0.428248</td>\n",
       "      <td>0.368278</td>\n",
       "      <td>0.423013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438494</td>\n",
       "      <td>0.402396</td>\n",
       "      <td>0.396254</td>\n",
       "      <td>0.425332</td>\n",
       "      <td>0.403918</td>\n",
       "      <td>0.440132</td>\n",
       "      <td>0.388455</td>\n",
       "      <td>0.378775</td>\n",
       "      <td>0.450683</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 to        of  pleasure      pain         a       the  \\\n",
       "to         0.000000  0.787391  0.810214  0.827404  0.797251  0.814888   \n",
       "of         0.787391  0.000000  0.384531  0.416828  0.325038  0.406923   \n",
       "pleasure   0.810214  0.384531  0.000000  0.409522  0.375739  0.403045   \n",
       "pain       0.827404  0.416828  0.409522  0.000000  0.391088  0.427218   \n",
       "a          0.797251  0.325038  0.375739  0.391088  0.000000  0.377490   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "enjoy      0.826989  0.398741  0.394687  0.378397  0.441894  0.410658   \n",
       "annoying   0.780721  0.408482  0.416837  0.351677  0.406800  0.438933   \n",
       "produces   0.832445  0.411565  0.403753  0.332776  0.377152  0.422260   \n",
       "resultant  0.785573  0.440255  0.441222  0.430496  0.433487  0.439111   \n",
       "PAD        0.789727  0.405608  0.411379  0.420382  0.366327  0.409461   \n",
       "\n",
       "                who       but       and        or  ...      find     fault  \\\n",
       "to         0.786212  0.853085  0.804344  0.820375  ...  0.778528  0.810712   \n",
       "of         0.388624  0.438588  0.389750  0.426883  ...  0.425372  0.401584   \n",
       "pleasure   0.407210  0.402263  0.393062  0.427232  ...  0.429522  0.404031   \n",
       "pain       0.427060  0.412869  0.367085  0.401413  ...  0.407849  0.381329   \n",
       "a          0.405434  0.418239  0.381655  0.385599  ...  0.419062  0.340630   \n",
       "...             ...       ...       ...       ...  ...       ...       ...   \n",
       "enjoy      0.391083  0.406363  0.407552  0.428812  ...  0.429945  0.439026   \n",
       "annoying   0.426726  0.446179  0.385946  0.389236  ...  0.415694  0.452983   \n",
       "produces   0.434561  0.405213  0.405003  0.406478  ...  0.415976  0.410710   \n",
       "resultant  0.429622  0.434378  0.363705  0.383129  ...  0.381278  0.406512   \n",
       "PAD        0.399728  0.428248  0.368278  0.423013  ...  0.438494  0.402396   \n",
       "\n",
       "               with       man   chooses     enjoy  annoying  produces  \\\n",
       "to         0.812098  0.789316  0.770183  0.826989  0.780721  0.832445   \n",
       "of         0.444521  0.409172  0.373554  0.398741  0.408482  0.411565   \n",
       "pleasure   0.399282  0.409312  0.405998  0.394687  0.416837  0.403753   \n",
       "pain       0.419598  0.357400  0.424888  0.378397  0.351677  0.332776   \n",
       "a          0.443746  0.377340  0.369278  0.441894  0.406800  0.377152   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "enjoy      0.420391  0.400164  0.428240  0.000000  0.424172  0.416142   \n",
       "annoying   0.443795  0.378387  0.435288  0.424172  0.000000  0.379482   \n",
       "produces   0.432731  0.432116  0.419045  0.416142  0.379482  0.000000   \n",
       "resultant  0.433371  0.401329  0.417583  0.399358  0.453137  0.425729   \n",
       "PAD        0.396254  0.425332  0.403918  0.440132  0.388455  0.378775   \n",
       "\n",
       "           resultant       PAD  \n",
       "to          0.785573  0.789727  \n",
       "of          0.440255  0.405608  \n",
       "pleasure    0.441222  0.411379  \n",
       "pain        0.430496  0.420382  \n",
       "a           0.433487  0.366327  \n",
       "...              ...       ...  \n",
       "enjoy       0.399358  0.440132  \n",
       "annoying    0.453137  0.388455  \n",
       "produces    0.425729  0.378775  \n",
       "resultant   0.000000  0.450683  \n",
       "PAD         0.450683  0.000000  \n",
       "\n",
       "[102 rows x 102 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "data = pd.DataFrame(distance_matrix,index=word2id.keys())\n",
    "data.columns = word2id.keys()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9b0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchWord(WordList):\n",
    "    similar_words ={}\n",
    "    for search_term in WordList:\n",
    "        if(search_term in word2id.keys()):\n",
    "            similar_words[search_term]=[id2word[idx] for idx in \n",
    "                                        distance_matrix[word2id[search_term]-1].argsort()[0:5]+1] \n",
    "    return similar_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec9bbe76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enjoy': ['enjoy', 'desires', 'one', 'again', 'know']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SearchWord(['enjoy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
